{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JyMz9Pj87Eq",
        "outputId": "2ccf539b-a47b-4164-e8f7-245547c6cf9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU3pvAFG8oC9",
        "outputId": "0a661b79-6ef7-451f-cafc-fb67aa11c4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/speechbrain/speechbrain.git@develop\n",
            "  Cloning https://github.com/speechbrain/speechbrain.git (to revision develop) to /tmp/pip-req-build-hjjq4f5x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/speechbrain/speechbrain.git /tmp/pip-req-build-hjjq4f5x\n",
            "  Resolved https://github.com/speechbrain/speechbrain.git to commit 1350e9b3cebae9f78e57e97d82a2d89ba3fc2ae1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hyperpyyaml (from speechbrain==1.0.0)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain==1.0.0) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain==1.0.0) (24.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain==1.0.0) (1.11.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain==1.0.0) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain==1.0.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain==1.0.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain==1.0.0) (4.66.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from speechbrain==1.0.0) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain==1.0.0) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain==1.0.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain==1.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain==1.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain==1.0.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain==1.0.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m949.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain==1.0.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->speechbrain==1.0.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain==1.0.0) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain==1.0.0) (6.0.1)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain==1.0.0)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain==1.0.0)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain==1.0.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain==1.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain==1.0.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain==1.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain==1.0.0) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain==1.0.0) (1.3.0)\n",
            "Building wheels for collected packages: speechbrain\n",
            "  Building wheel for speechbrain (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for speechbrain: filename=speechbrain-1.0.0-py3-none-any.whl size=787781 sha256=180e4cb8d471dbfa4b005cffbfdf0ebf3a25f6387b238b8698ec8bc9c8779f20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9jnagkli/wheels/e1/a5/ca/79fbb0a28f6f392850cfd5bac6a4dcec0f10a7e49d4c1e87cb\n",
            "Successfully built speechbrain\n",
            "Installing collected packages: ruamel.yaml.clib, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, hyperpyyaml, speechbrain\n",
            "Successfully installed hyperpyyaml-1.2.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 speechbrain-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/speechbrain/speechbrain.git@develop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchaudio._internal import download_url_to_file\n",
        "from torchaudio.datasets.utils import _extract_zip, _load_waveform"
      ],
      "metadata": {
        "id": "HYqw-2YZCDue"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 16000\n",
        "_ARCHIVE_CONFIGS = {\n",
        "    # \"dev\": {\n",
        "    #     \"archive_name\": \"vox1_dev_wav.zip\",\n",
        "    #     \"urls\": [\n",
        "    #         \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\",\n",
        "    #         \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\",\n",
        "    #         \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\",\n",
        "    #         \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\",\n",
        "    #     ],\n",
        "    #     \"checksums\": [\n",
        "    #         \"21ec6ca843659ebc2fdbe04b530baa4f191ad4b0971912672d92c158f32226a0\",\n",
        "    #         \"311d21e0c8cbf33573a4fce6c80e5a279d80736274b381c394319fc557159a04\",\n",
        "    #         \"92b64465f2b2a3dc0e4196ae8dd6828cbe9ddd1f089419a11e4cbfe2e1750df0\",\n",
        "    #         \"00e6190c770b27f27d2a3dd26ee15596b17066b715ac111906861a7d09a211a5\",\n",
        "    #     ],\n",
        "    # },\n",
        "    \"test\": {\n",
        "        \"archive_name\": \"vox1_test_wav.zip\",\n",
        "        \"url\": \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\",\n",
        "        \"checksum\": \"8de57f347fe22b2c24526e9f444f689ecf5096fc2a92018cf420ff6b5b15eaea\",\n",
        "    },\n",
        "}\n",
        "_IDEN_SPLIT_URL = \"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/iden_split.txt\"\n",
        "_VERI_TEST_URL = \"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test.txt\"\n",
        "\n",
        "\n",
        "def _download_extract_wavs(root: str):\n",
        "    for archive in [\"test\"]:\n",
        "        archive_name = _ARCHIVE_CONFIGS[archive][\"archive_name\"]\n",
        "        archive_path = os.path.join(root, archive_name)\n",
        "        # The zip file of dev data is splited to 4 chunks.\n",
        "        # Download and combine them into one file before extraction.\n",
        "        if archive == \"dev\":\n",
        "            urls = _ARCHIVE_CONFIGS[archive][\"urls\"]\n",
        "            checksums = _ARCHIVE_CONFIGS[archive][\"checksums\"]\n",
        "            with open(archive_path, \"wb\") as f:\n",
        "                for url, checksum in zip(urls, checksums):\n",
        "                    file_path = os.path.join(root, os.path.basename(url))\n",
        "                    download_url_to_file(url, file_path, hash_prefix=checksum)\n",
        "                    with open(file_path, \"rb\") as f_split:\n",
        "                        f.write(f_split.read())\n",
        "        else:\n",
        "            url = _ARCHIVE_CONFIGS[archive][\"url\"]\n",
        "            checksum = _ARCHIVE_CONFIGS[archive][\"checksum\"]\n",
        "            download_url_to_file(url, archive_path, hash_prefix=checksum)\n",
        "        _extract_zip(archive_path)\n",
        "\n",
        "\n",
        "def _get_flist(root: str, file_path: str, subset: str) -> List[str]:\n",
        "    f_list = []\n",
        "    if subset == \"train\":\n",
        "        index = 1\n",
        "    elif subset == \"dev\":\n",
        "        index = 2\n",
        "    else:\n",
        "        index = 3\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            id, path = line.split()\n",
        "            if int(id) == index:\n",
        "                f_list.append(path)\n",
        "    return sorted(f_list)\n",
        "\n",
        "\n",
        "def _get_paired_flist(root: str, veri_test_path: str):\n",
        "    f_list = []\n",
        "    with open(veri_test_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            label, path1, path2 = line.split()\n",
        "            f_list.append((label, path1, path2))\n",
        "    return f_list\n",
        "\n",
        "\n",
        "def _get_file_id(file_path: str, _ext_audio: str):\n",
        "    speaker_id, youtube_id, utterance_id = file_path.split(\"/\")[-3:]\n",
        "    utterance_id = utterance_id.replace(_ext_audio, \"\")\n",
        "    file_id = \"-\".join([speaker_id, youtube_id, utterance_id])\n",
        "    return file_id\n"
      ],
      "metadata": {
        "id": "tNOB-aNMCWnu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VoxCeleb1(Dataset):\n",
        "    \"\"\"*VoxCeleb1* :cite:`nagrani2017voxceleb` dataset.\n",
        "\n",
        "    Args:\n",
        "        root (str or Path): Path to the directory where the dataset is found or downloaded.\n",
        "        download (bool, optional):\n",
        "            Whether to download the dataset if it is not found at root path. (Default: ``False``).\n",
        "    \"\"\"\n",
        "\n",
        "    _ext_audio = \".wav\"\n",
        "\n",
        "    def __init__(self, root: Union[str, Path], download: bool = False) -> None:\n",
        "        # Get string representation of 'root' in case Path object is passed\n",
        "        root = os.fspath(root)\n",
        "        self._path = os.path.join(root, \"wav\")\n",
        "        if not os.path.isdir(self._path):\n",
        "            if not download:\n",
        "                raise RuntimeError(\n",
        "                    f\"Dataset not found at {self._path}. Please set `download=True` to download the dataset.\"\n",
        "                )\n",
        "            _download_extract_wavs(root)\n",
        "\n",
        "    def get_metadata(self, n: int):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, n: int):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "ize-CMjBCfbw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VoxCeleb1Verification(VoxCeleb1):\n",
        "    \"\"\"*VoxCeleb1* :cite:`nagrani2017voxceleb` dataset for speaker verification task.\n",
        "\n",
        "    Each data sample contains a pair of waveforms, sample rate, the label indicating if they are\n",
        "    from the same speaker, and the file ids.\n",
        "\n",
        "    Args:\n",
        "        root (str or Path): Path to the directory where the dataset is found or downloaded.\n",
        "        meta_url (str, optional): The url of meta file that contains a list of utterance pairs\n",
        "            and the corresponding labels. The format of each row is ``label file_path1 file_path2\".\n",
        "            For example: ``1 id10270/x6uYqmx31kE/00001.wav id10270/8jEAjG6SegY/00008.wav``.\n",
        "            ``1`` means the two utterances are from the same speaker, ``0`` means not.\n",
        "            (Default: ``\"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test.txt\"``)\n",
        "        download (bool, optional):\n",
        "            Whether to download the dataset if it is not found at root path. (Default: ``False``).\n",
        "\n",
        "    Note:\n",
        "        The file structure of `VoxCeleb1Verification` dataset is as follows:\n",
        "\n",
        "        └─ root/\n",
        "\n",
        "         └─ wav/\n",
        "\n",
        "         └─ speaker_id folders\n",
        "\n",
        "        Users who pre-downloaded the ``\"vox1_dev_wav.zip\"`` and ``\"vox1_test_wav.zip\"`` files need to move\n",
        "        the extracted files into the same ``root`` directory.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: Union[str, Path], meta_url: str = _VERI_TEST_URL, download: bool = False) -> None:\n",
        "        super().__init__(root, download)\n",
        "        # download the veri_test.txt to get the list of training pairs and labels.\n",
        "        meta_list_path = os.path.join(root, os.path.basename(meta_url))\n",
        "        if not os.path.exists(meta_list_path):\n",
        "            download_url_to_file(meta_url, meta_list_path)\n",
        "        self._flist = _get_paired_flist(self._path, meta_list_path)\n",
        "\n",
        "    def get_metadata(self, n: int) -> Tuple[str, str, int, int, str, str]:\n",
        "        \"\"\"Get metadata for the n-th sample from the dataset. Returns filepaths instead of waveforms,\n",
        "        but otherwise returns the same fields as :py:func:`__getitem__`.\n",
        "\n",
        "        Args:\n",
        "            n (int): The index of the sample\n",
        "\n",
        "        Returns:\n",
        "            Tuple of the following items;\n",
        "\n",
        "            str:\n",
        "                Path to audio file of speaker 1\n",
        "            str:\n",
        "                Path to audio file of speaker 2\n",
        "            int:\n",
        "                Sample rate\n",
        "            int:\n",
        "                Label\n",
        "            str:\n",
        "                File ID of speaker 1\n",
        "            str:\n",
        "                File ID of speaker 2\n",
        "        \"\"\"\n",
        "        label, file_path_spk1, file_path_spk2 = self._flist[n]\n",
        "        label = int(label)\n",
        "        file_id_spk1 = _get_file_id(file_path_spk1, self._ext_audio)\n",
        "        file_id_spk2 = _get_file_id(file_path_spk2, self._ext_audio)\n",
        "        return file_path_spk1, file_path_spk2, SAMPLE_RATE, label, file_id_spk1, file_id_spk2\n",
        "\n",
        "\n",
        "    def __getitem__(self, n: int) -> Tuple[Tensor, Tensor, int, int, str, str]:\n",
        "        \"\"\"Load the n-th sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            n (int): The index of the sample to be loaded.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of the following items;\n",
        "\n",
        "            Tensor:\n",
        "                Waveform of speaker 1\n",
        "            Tensor:\n",
        "                Waveform of speaker 2\n",
        "            int:\n",
        "                Sample rate\n",
        "            int:\n",
        "                Label\n",
        "            str:\n",
        "                File ID of speaker 1\n",
        "            str:\n",
        "                File ID of speaker 2\n",
        "        \"\"\"\n",
        "        metadata = self.get_metadata(n)\n",
        "        waveform_spk1 = _load_waveform(self._path, metadata[0], metadata[2])\n",
        "        waveform_spk2 = _load_waveform(self._path, metadata[1], metadata[2])\n",
        "        return (waveform_spk1, waveform_spk2) + metadata[2:]\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._flist)"
      ],
      "metadata": {
        "id": "89IIhZF9B0J2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nLO0MW-5CVMq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJrCZuPCmqvs",
        "outputId": "2c8264da-904a-45b4-cb94-fff964143c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.00G/1.00G [00:34<00:00, 31.0MB/s]\n",
            "100%|██████████| 2.23M/2.23M [00:00<00:00, 3.68MB/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = VoxCeleb1Verification(root=\"/content/\", download = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LNyCiYNimype",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9181ffc5-5f54-4a3b-f309-a3fa6ead13b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('id10270/x6uYqmx31kE/00001.wav',\n",
              " 'id10270/8jEAjG6SegY/00008.wav',\n",
              " 16000,\n",
              " 1,\n",
              " 'id10270-x6uYqmx31kE-00001',\n",
              " 'id10270-8jEAjG6SegY-00008')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset.get_metadata(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "_dJ41H9WtYEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096fbe10-cbad-4fb2-b599-5980f5efc123"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0068,  0.0064,  0.0049,  ..., -0.0047, -0.0041, -0.0033]]),\n",
              " tensor([[-0.0105, -0.0110, -0.0120,  ...,  0.0349,  0.0309,  0.0255]]),\n",
              " 16000,\n",
              " 1,\n",
              " 'id10270-x6uYqmx31kE-00001',\n",
              " 'id10270-8jEAjG6SegY-00008')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"/content/wav/\" + dataset[0][4].replace(\"-\", \"/\") + \".wav\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sgdWz7M0FL-B",
        "outputId": "8a560a3a-2795-4af3-9e2d-f4765cfc2c8f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/wav/id10270/x6uYqmx31kE/00001.wav'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6W-oErMhAzM"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "from speechbrain.inference.speaker import EncoderClassifier\n",
        "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", run_opts={\"device\":\"cuda\"})\n",
        "signal, fs =torchaudio.load('/content/Aadiksha_6.wav')\n",
        "embeddings = classifier.encode_batch(signal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m-w94vyhFRl",
        "outputId": "1777f306-6713-4951-d963-f1e806c7ce05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.5862]), tensor([True]))\n"
          ]
        }
      ],
      "source": [
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
        "print(verification.verify_files(\"/content/wav/\" + dataset[0][4].replace(\"-\", \"/\") + \".wav\",\"/content/wav/\" + dataset[0][5].replace(\"-\", \"/\") + \".wav\")) # Different Speakers\n",
        "# print(verification.verify_files(\"/content/NarendraModi_1.wav\", \"/content/NarendraModi_2.wav\")) # Same Speaker\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_oiHSBcs-yR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}